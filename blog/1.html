<!DOCTYPE html><html lang="en" class="dark bg-neutral-950"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/navigation.gif"/><link rel="preload" as="image" href="/rhib-splash.jpg"/><link rel="preload" as="image" href="/avns_assets/sim.png"/><link rel="preload" as="image" href="/avns_assets/robot_navigator.py.png"/><link rel="preload" as="image" href="/avns_assets/gpswaypointfollower.py.png"/><link rel="preload" as="image" href="/avns_assets/Multiple_object_tracking_lidar.cpp.png"/><link rel="preload" as="image" href="/avns_assets/before.gif"/><link rel="preload" as="image" href="/avns_assets/after.gif"/><link rel="preload" as="image" href="/avns_assets/Nav2_params_costmap.yaml.png"/><link rel="preload" as="image" href="/avns_assets/Nav2_params_DWB.yaml.png"/><link rel="stylesheet" href="/_next/static/css/341a90bf799da2ca.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/d4c16f7ed518c08c.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-0be3a2bebbb00bbd.js"/><script src="/_next/static/chunks/fd9d1056-07fef362c3bed963.js" async=""></script><script src="/_next/static/chunks/23-b8d8687b14009d7a.js" async=""></script><script src="/_next/static/chunks/main-app-3d62143065e3cac3.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-2998c1456e879c31.js" async=""></script><script src="/_next/static/chunks/270-1ffccbd649a0d4c1.js" async=""></script><script src="/_next/static/chunks/app/layout-b0520be3c5ae1222.js" async=""></script><title>Create T3 App</title><meta name="description" content="Generated by create-t3-app"/><link rel="icon" href="/favicon.ico"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body><div class="absolute top-2 left-0"><div class="terminal"><pre><div><a href="#">├──<!-- --> <!-- -->.</a></div></pre></div></div><div class="bg-neutral-950 min-h-screen flex justify-center items-center "><div class="bg-neutral-950 p-8 max-w-3xl mx-auto rounded shadow-lg text-white font-mono"><div class="toc" style="position:fixed;right:0;top:0;width:200px;height:100%;overflow:auto;padding:20px;font-family:Arial, sans-serif;background-color:#18181b">
  <style>
  .toc a {
    color: #cbd5e1; /* Change this to your preferred color */
    text-decoration: none;
  }

  .toc a:hover {
    text-decoration: underline;
    color: #475569; /* Change this to your preferred color */
  }

  @media (max-width: 1100px) {
    .toc {
      display: none;
    }
  }
  </style>
<ol>
<li><a href="#introduction">introduction</a></li>
<li><a href="#platform">the platform</a></li>
<li><a href="#ros">ros</a></li>
<li><a href="#local">localization</a></li>
<li><a href="#waypoint">waypoints</a></li>
<li><a href="#tuning">tuning</a></li>
<li><a href="#acknowledgements">acknowledgements</a></li>
</ol>
</div>
<div id="introduction" style="margin-bottom:20px">
  <h1 style="margin-bottom:20px;font-size:2em">USV Navigation</h1>
</div>
<div style="margin-top:10px">
    Disclaimer: This project was my introduction to robotics. The methods described may not be optimal or technically correct, but they were effective for my specific application. This is a high level overview, not everything will be covered.
</div>
<div style="display:flex;flex-direction:column;align-items:center;text-align:center;margin-bottom:20px">
  <img src="/navigation.gif" alt="Navigation" style="width:400px;border-radius:2px"/>
  <figcaption style="font-style:italic">An example of close quarters navigation between GPS waypoints.</figcaption>
</div>
<div id="introduction" style="margin-top:20px">
  In this post I want to explain the architecture of our USV navigation system built for the OSU College of Earth Ocean and Atmospherics Ocean Mixing group, show you how it performed, and hopefully help anyone who finds themselves trying to throw together a robotics project that is going to employ some type of autonomous navigation (pretty sure thats most of them).
</div>
<div style="margin-top:20px">
 The Ocean Mixing group measures fresh-salt water interactions (mixing) to understand the complex dynamics of fluid motion on a geophysical scale. By studying the mixing processes in the ocean, which include the interplay of ocean currents, waves, and turbulence, the group aims to elucidate how mass, momentum, and heat are distributed and transported within the ocean. Im not going to pretend that I understand how this works, but naturally this type of measurement needs to occur somewhere fresh water and salt water meet: 
 <div style="display:flex;flex-direction:column;align-items:center;text-align:center;margin-bottom:20px">
  <img src="/rhib-splash.jpg" alt="splash" style="width:500px;border-radius:2px"/>
  <figcaption style="font-style:italic">Leconte Glacier, Alaska. You can see why it needs to be remotely operated/autonomous.</figcaption>
</div>
</div>
<div id="sim" style="display:flex;flex-direction:column;align-items:center;text-align:center;margin-top:20px">
The simulation environment used for this project is the Virtual RobotX marine simulator. Minimal changes were made to the world sdf file included in the sim. These include changes to the wave dynamics (increase height, frequency), changes to the model, hull physics, and positioning of sensors of the boat to more closely resemble the real world operating conditions, and some imported and retextured models of sea rocks to look like icebergs. The simulator already publishes several common USV sensors such as GPS, IMU, and lidar.
  <img src="/avns_assets/sim.png" alt="code" style="width:400px;max-width:none;border-radius:2px"/>
</div>
<div id="ros" style="margin-top:20px">
 Robot Operating System is an open source framework for developing robotics software. It contains middleware, tools, libraries, package management, and drivers that are all super useful for people making robotics software. This project uses the very popular ROS packages robot_localization and navigation2. Robot_localizaiton is used for sensor fusion and state estimation of the usv (taking various position/angle estimates from sensors and fusing them into one ultimate position/angle estimate) and the Nav2 package provides a whole ecosystem of navigation related tools including mapping, path planning algorithms, behavior trees, and more.
 </div>
<div id="localization" style="margin-top:20px">
Robot_localization is going to fuse our sensor measurements together into a global odometry message. We need absolute position X and Y from our GPS (we assume that absolute Z is 0), x acceleration and yaw position from our IMU. There&#x27;s a lot of parameters not mentioned, you can check the configuration out in the file ekf_with_gps.yaml on the github.
</div>
<div id="waypoints" style="margin-top:20px">
Nav2 provides the methods through their Simple Commander API to interact with the navigation system. Unfortunately, the Nav2 library does not currently implement gps waypoint following capability, but it isn&#x27;t too difficult to add. With the ROS2 build system, you can pull a specific file from a package into your workspace/development environment to make changes to it. In this instance, we want to change the Robot_Navigator.py file. This file contains the definition of the Basic_navigator class which the Simple Commander API implements. Here, we will change the following subscribed topic:
<div style="display:flex;flex-direction:column;align-items:center;text-align:center;margin-bottom:20px">
  <img src="/avns_assets/robot_navigator.py.png" alt="code" style="width:700px;max-width:none;border-radius:2px"/>
  <!-- -->
  This topic subscription (amcl_pose) actually just takes a global pose with covariance, so it should be ok to resubscribe it to our robot_localization global_ekf. Note that the global ekf topic published by robot_localization is an odometry message, so I have extracted the pose and republished it separately.
</div>
</div>
<div style="display:flex;flex-direction:column;align-items:center;text-align:center;margin-bottom:20px">
    Now, we can create a fairly simple python script that reads in a list of gps waypoints, converts them to map points, and then calls the simple commander api to trigger navigation:
    <img src="/avns_assets/gpswaypointfollower.py.png" alt="code" style="width:700px;max-width:none;border-radius:2px"/>
</div>
<div id="tuning" style="margin-top:20px">
I encountered several issues in the marine simulation. Firstly, the 2D lidar is mounted on the USV about a meter above the water. Since the lidar emits a two-dimensional beam, it cannot detect objects that are either below or above its beam path. Many pieces of marine debris are smaller than the boat and just barely protrude above the water&#x27;s surface. To address this, the lidar is mounted at an angle so that the beam intersects the water&#x27;s surface ahead of the boat, allowing it to detect small objects. However, this setup causes the water surface to appear as an obstacle, which contaminates the cost map and confuses the path planning algorithm into thinking there are no navigable paths.
</div>
<div id="tuning" style="margin-top:20px">
To resolve this, I adjusted the maximum obstacle range on the cost map to just before where the beam hits the water surface. Based on the sensor&#x27;s angle and height, I calculated this distance to be around 16 meters. This solution was imperfect, as it assumed a completely flat water surface, but the boat&#x27;s motion and wave peaks caused additional complications. Reducing the range further wasn&#x27;t ideal, as I wanted the boat to have enough time to avoid obstacles. Thus, I needed a way to filter out the wave patterns that were being misinterpreted as objects.
</div>
<div id="tuning" style="margin-top:20px">
My teammate, Parker Carlson, did an excellent job refactoring and porting the multiple-object-tracking-lidar package written by Praveen Palanisamy. This package leverages Kalman filtering and k-means clustering to more accurately detect real objects in the environment, allowing us to ignore the noise from the waves. I contributed by creating a rotation matrix based on the IMU-reported angle. With this lidar filter in place, the wave patterns were largely excluded from the central field of view, enabling me to maintain the cost map&#x27;s object range at 16 meters.
  <img src="/avns_assets/Multiple_object_tracking_lidar.cpp.png" alt="code" style="width:700px;max-width:none;border-radius:2px"/>
</div>
<div id="test" style="margin-top:20px;display:flex;justify-content:space-between">
Before and after multiple_object_tracking lidar filter. Further improved by pointcloud correction with IMU data.
</div>
<div id="test" style="margin-top:20px;display:flex;justify-content:space-between">
<img src="/avns_assets/before.gif" alt="code" style="width:300px;max-width:none;border-radius:2px"/>
<img src="/avns_assets/after.gif" alt="code" style="width:300px;max-width:none;border-radius:2px"/>
</div>
<div id="tuning" style="margin-top:20px">
Finally, we adjusted some parameters of Nav2 for optimal performance, while leaving most settings at their default values. One key adjustment was to the costmap raytracing range. In Nav2, costmap2D uses raytracing to determine if an object should be removed from the costmap, and this can be configured with minimum and maximum ranges. We set the upper range to 16 meters, which matches our lidar range since the lidar beam doesn&#x27;t effectively reach beyond 16 meters without encountering the water.
</div>
<div id="tuning" style="margin-top:20px">
Additionally, since the lidar is positioned approximately 1 meter above the water, it cannot reliably detect smaller objects that are close to and below the sensor. To account for this, we set the minimum raytracing range parameter, ensuring that objects within this range aren&#x27;t prematurely cleared from the costmap. Through experimentation, we found that 7 meters is an effective setting.
</div>
<div id="test" style="margin-top:20px;display:flex;justify-content:center">
<img src="/avns_assets/Nav2_params_costmap.yaml.png" alt="code" style="width:450px;max-width:none;border-radius:2px"/>
</div>
<div id="tuning" style="margin-top:20px">
Another important set of parameters are the PathAlign.scale and GoalAlign.scale. These settings allow the robot to optimize its path around objects, creating smoother and more direct navigation. However, in my case, these settings proved problematic. They resulted in the USV taking shortcuts that brought it too close to obstacles, sometimes leading to collisions. Here I set them to zero:
</div>
<div id="test" style="margin-top:20px;display:flex;justify-content:space-between">
<img src="/avns_assets/Nav2_params_DWB.yaml.png" alt="code" style="width:700px;max-width:none;border-radius:2px"/>
</div>
<div id="acknowledgements" style="margin-top:20px">
This project would not have been possible without the invaluable help of my co-authors, Parker Carlson and Ethan Cline.
</div></div></div><script src="/_next/static/chunks/webpack-0be3a2bebbb00bbd.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/341a90bf799da2ca.css\",\"style\"]\n2:HL[\"/_next/static/css/d4c16f7ed518c08c.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n5:I[376,[\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-2998c1456e879c31.js\"],\"default\"]\n6:I[9275,[],\"\"]\n8:I[1343,[],\"\"]\n9:I[6823,[\"270\",\"static/chunks/270-1ffccbd649a0d4c1.js\",\"185\",\"static/chunks/app/layout-b0520be3c5ae1222.js\"],\"TRPCReactProvider\"]\nb:I[6130,[],\"\"]\n7:[\"slug\",\"1\",\"d\"]\nc:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/341a90bf799da2ca.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"7tGa0WZtUPXrJDH5uGfL7\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/1\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"1\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"1\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"1\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",[[\"$\",\"div\",null,{\"className\":\"absolute top-2 left-0\",\"children\":[\"$\",\"$L5\",null,{}]}],[\"$\",\"div\",null,{\"className\":\"bg-neutral-950 min-h-screen flex justify-center items-center \",\"children\":[\"$\",\"div\",null,{\"className\":\"bg-neutral-950 p-8 max-w-3xl mx-auto rounded shadow-lg text-white font-mono\",\"children\":[[\"$\",\"div\",\"div-0\",{\"className\":\"toc\",\"style\":{\"position\":\"fixed\",\"right\":\"0\",\"top\":\"0\",\"width\":\"200px\",\"height\":\"100%\",\"overflow\":\"auto\",\"padding\":\"20px\",\"fontFamily\":\"Arial, sans-serif\",\"backgroundColor\":\"#18181b\"},\"children\":[\"\\n  \",[\"$\",\"style\",\"style-0\",{\"children\":\"\\n  .toc a {\\n    color: #cbd5e1; /* Change this to your preferred color */\\n    text-decoration: none;\\n  }\\n\\n  .toc a:hover {\\n    text-decoration: underline;\\n    color: #475569; /* Change this to your preferred color */\\n  }\\n\\n  @media (max-width: 1100px) {\\n    .toc {\\n      display: none;\\n    }\\n  }\\n  \"}],\"\\n\",[\"$\",\"ol\",\"ol-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"#introduction\",\"children\":\"introduction\"}]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"#platform\",\"children\":\"the platform\"}]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"#ros\",\"children\":\"ros\"}]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"#local\",\"children\":\"localization\"}]}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"#waypoint\",\"children\":\"waypoints\"}]}],\"\\n\",[\"$\",\"li\",\"li-5\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"#tuning\",\"children\":\"tuning\"}]}],\"\\n\",[\"$\",\"li\",\"li-6\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"#acknowledgements\",\"children\":\"acknowledgements\"}]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-1\",{\"id\":\"introduction\",\"style\":{\"marginBottom\":\"20px\"},\"children\":[\"\\n  \",[\"$\",\"h1\",\"h1-0\",{\"style\":{\"marginBottom\":\"20px\",\"fontSize\":\"2em\"},\"children\":\"USV Navigation\"}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-2\",{\"id\\\"\\\"\":\"\",\"style\":{\"marginTop\":\"10px\"},\"children\":\"\\n    Disclaimer: This project was my introduction to robotics. The methods described may not be optimal or technically correct, but they were effective for my specific application. This is a high level overview, not everything will be covered.\\n\"}],\"\\n\",[\"$\",\"div\",\"div-3\",{\"style\":{\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"textAlign\":\"center\",\"marginBottom\":\"20px\"},\"children\":[\"\\n  \",[\"$\",\"img\",\"img-0\",{\"src\":\"/navigation.gif\",\"alt\":\"Navigation\",\"style\":{\"width\":\"400px\",\"borderRadius\":\"2px\"}}],\"\\n  \",[\"$\",\"figcaption\",\"figcaption-0\",{\"style\":{\"fontStyle\":\"italic\"},\"children\":\"An example of close quarters navigation between GPS waypoints.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-4\",{\"id\":\"introduction\",\"style\":{\"marginTop\":\"20px\"},\"children\":\"\\n  In this post I want to explain the architecture of our USV navigation system built for the OSU College of Earth Ocean and Atmospherics Ocean Mixing group, show you how it performed, and hopefully help anyone who finds themselves trying to throw together a robotics project that is going to employ some type of autonomous navigation (pretty sure thats most of them).\\n\"}],\"\\n\",[\"$\",\"div\",\"div-5\",{\"style\":{\"marginTop\":\"20px\"},\"children\":[\"\\n The Ocean Mixing group measures fresh-salt water interactions (mixing) to understand the complex dynamics of fluid motion on a geophysical scale. By studying the mixing processes in the ocean, which include the interplay of ocean currents, waves, and turbulence, the group aims to elucidate how mass, momentum, and heat are distributed and transported within the ocean. Im not going to pretend that I understand how this works, but naturally this type of measurement needs to occur somewhere fresh water and salt water meet: \\n \",[\"$\",\"div\",\"div-0\",{\"style\":{\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"textAlign\":\"center\",\"marginBottom\":\"20px\"},\"children\":[\"\\n  \",[\"$\",\"img\",\"img-0\",{\"src\":\"/rhib-splash.jpg\",\"alt\":\"splash\",\"style\":{\"width\":\"500px\",\"borderRadius\":\"2px\"}}],\"\\n  \",[\"$\",\"figcaption\",\"figcaption-0\",{\"style\":{\"fontStyle\":\"italic\"},\"children\":\"Leconte Glacier, Alaska. You can see why it needs to be remotely operated/autonomous.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-6\",{\"id\":\"sim\",\"style\":{\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"textAlign\":\"center\",\"marginTop\":\"20px\"},\"children\":[\"\\nThe simulation environment used for this project is the Virtual RobotX marine simulator. Minimal changes were made to the world sdf file included in the sim. These include changes to the wave dynamics (increase height, frequency), changes to the model, hull physics, and positioning of sensors of the boat to more closely resemble the real world operating conditions, and some imported and retextured models of sea rocks to look like icebergs. The simulator already publishes several common USV sensors such as GPS, IMU, and lidar.\\n  \",[\"$\",\"img\",\"img-0\",{\"src\":\"/avns_assets/sim.png\",\"alt\":\"code\",\"style\":{\"width\":\"400px\",\"maxWidth\":\"none\",\"borderRadius\":\"2px\"}}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-7\",{\"id\":\"ros\",\"style\":{\"marginTop\":\"20px\"},\"children\":\"\\n Robot Operating System is an open source framework for developing robotics software. It contains middleware, tools, libraries, package management, and drivers that are all super useful for people making robotics software. This project uses the very popular ROS packages robot_localization and navigation2. Robot_localizaiton is used for sensor fusion and state estimation of the usv (taking various position/angle estimates from sensors and fusing them into one ultimate position/angle estimate) and the Nav2 package provides a whole ecosystem of navigation related tools including mapping, path planning algorithms, behavior trees, and more.\\n \"}],\"\\n\",[\"$\",\"div\",\"div-8\",{\"id\":\"localization\",\"style\":{\"marginTop\":\"20px\"},\"children\":\"\\nRobot_localization is going to fuse our sensor measurements together into a global odometry message. We need absolute position X and Y from our GPS (we assume that absolute Z is 0), x acceleration and yaw position from our IMU. There's a lot of parameters not mentioned, you can check the configuration out in the file ekf_with_gps.yaml on the github.\\n\"}],\"\\n\",[\"$\",\"div\",\"div-9\",{\"id\":\"waypoints\",\"style\":{\"marginTop\":\"20px\"},\"children\":[\"\\nNav2 provides the methods through their Simple Commander API to interact with the navigation system. Unfortunately, the Nav2 library does not currently implement gps waypoint following capability, but it isn't too difficult to add. With the ROS2 build system, you can pull a specific file from a package into your workspace/development environment to make changes to it. In this instance, we want to change the Robot_Navigator.py file. This file contains the definition of the Basic_navigator class which the Simple Commander API implements. Here, we will change the following subscribed topic:\\n\",[\"$\",\"div\",\"div-0\",{\"style\":{\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"textAlign\":\"center\",\"marginBottom\":\"20px\"},\"children\":[\"\\n  \",[\"$\",\"img\",\"img-0\",{\"src\":\"/avns_assets/robot_navigator.py.png\",\"alt\":\"code\",\"style\":{\"width\":\"700px\",\"maxWidth\":\"none\",\"borderRadius\":\"2px\"}}],\"\\n  \",\"\\n  This topic subscription (amcl_pose) actually just takes a global pose with covariance, so it should be ok to resubscribe it to our robot_localization global_ekf. Note that the global ekf topic published by robot_localization is an odometry message, so I have extracted the pose and republished it separately.\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-10\",{\"style\":{\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"textAlign\":\"center\",\"marginBottom\":\"20px\"},\"children\":[\"\\n    Now, we can create a fairly simple python script that reads in a list of gps waypoints, converts them to map points, and then calls the simple commander api to trigger navigation:\\n    \",[\"$\",\"img\",\"img-0\",{\"src\":\"/avns_assets/gpswaypointfollower.py.png\",\"alt\":\"code\",\"style\":{\"width\":\"700px\",\"maxWidth\":\"none\",\"borderRadius\":\"2px\"}}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-11\",{\"id\":\"tuning\",\"style\":{\"marginTop\":\"20px\"},\"children\":\"\\nI encountered several issues in the marine simulation. Firstly, the 2D lidar is mounted on the USV about a meter above the water. Since the lidar emits a two-dimensional beam, it cannot detect objects that are either below or above its beam path. Many pieces of marine debris are smaller than the boat and just barely protrude above the water's surface. To address this, the lidar is mounted at an angle so that the beam intersects the water's surface ahead of the boat, allowing it to detect small objects. However, this setup causes the water surface to appear as an obstacle, which contaminates the cost map and confuses the path planning algorithm into thinking there are no navigable paths.\\n\"}],\"\\n\",[\"$\",\"div\",\"div-12\",{\"id\":\"tuning\",\"style\":{\"marginTop\":\"20px\"},\"children\":\"\\nTo resolve this, I adjusted the maximum obstacle range on the cost map to just before where the beam hits the water surface. Based on the sensor's angle and height, I calculated this distance to be around 16 meters. This solution was imperfect, as it assumed a completely flat water surface, but the boat's motion and wave peaks caused additional complications. Reducing the range further wasn't ideal, as I wanted the boat to have enough time to avoid obstacles. Thus, I needed a way to filter out the wave patterns that were being misinterpreted as objects.\\n\"}],\"\\n\",[\"$\",\"div\",\"div-13\",{\"id\":\"tuning\",\"style\":{\"marginTop\":\"20px\"},\"children\":[\"\\nMy teammate, Parker Carlson, did an excellent job refactoring and porting the multiple-object-tracking-lidar package written by Praveen Palanisamy. This package leverages Kalman filtering and k-means clustering to more accurately detect real objects in the environment, allowing us to ignore the noise from the waves. I contributed by creating a rotation matrix based on the IMU-reported angle. With this lidar filter in place, the wave patterns were largely excluded from the central field of view, enabling me to maintain the cost map's object range at 16 meters.\\n  \",[\"$\",\"img\",\"img-0\",{\"src\":\"/avns_assets/Multiple_object_tracking_lidar.cpp.png\",\"alt\":\"code\",\"style\":{\"width\":\"700px\",\"maxWidth\":\"none\",\"borderRadius\":\"2px\"}}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-14\",{\"id\":\"test\",\"style\":{\"marginTop\":\"20px\",\"display\":\"flex\",\"justifyContent\":\"space-between\"},\"children\":\"\\nBefore and after multiple_object_tracking lidar filter. Further improved by pointcloud correction with IMU data.\\n\"}],\"\\n\",[\"$\",\"div\",\"div-15\",{\"id\":\"test\",\"style\":{\"marginTop\":\"20px\",\"display\":\"flex\",\"justifyContent\":\"space-between\"},\"children\":[\"\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"/avns_assets/before.gif\",\"alt\":\"code\",\"style\":{\"width\":\"300px\",\"maxWidth\":\"none\",\"borderRadius\":\"2px\"}}],\"\\n\",[\"$\",\"img\",\"img-1\",{\"src\":\"/avns_assets/after.gif\",\"alt\":\"code\",\"style\":{\"width\":\"300px\",\"maxWidth\":\"none\",\"borderRadius\":\"2px\"}}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-16\",{\"id\":\"tuning\",\"style\":{\"marginTop\":\"20px\"},\"children\":\"\\nFinally, we adjusted some parameters of Nav2 for optimal performance, while leaving most settings at their default values. One key adjustment was to the costmap raytracing range. In Nav2, costmap2D uses raytracing to determine if an object should be removed from the costmap, and this can be configured with minimum and maximum ranges. We set the upper range to 16 meters, which matches our lidar range since the lidar beam doesn't effectively reach beyond 16 meters without encountering the water.\\n\"}],\"\\n\",[\"$\",\"div\",\"div-17\",{\"id\":\"tuning\",\"style\":{\"marginTop\":\"20px\"},\"children\":\"\\nAdditionally, since the lidar is positioned approximately 1 meter above the water, it cannot reliably detect smaller objects that are close to and below the sensor. To account for this, we set the minimum raytracing range parameter, ensuring that objects within this range aren't prematurely cleared from the costmap. Through experimentation, we found that 7 meters is an effective setting.\\n\"}],\"\\n\",[\"$\",\"div\",\"div-18\",{\"id\":\"test\",\"style\":{\"marginTop\":\"20px\",\"display\":\"flex\",\"justifyContent\":\"center\"},\"children\":[\"\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"/avns_assets/Nav2_params_costmap.yaml.png\",\"alt\":\"code\",\"style\":{\"width\":\"450px\",\"maxWidth\":\"none\",\"borderRadius\":\"2px\"}}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-19\",{\"id\":\"tuning\",\"style\":{\"marginTop\":\"20px\"},\"children\":\"\\nAnother important set of parameters are the PathAlign.scale and GoalAlign.scale. These settings allow the robot to optimize its path around objects, creating smoother and more direct navigation. However, in my case, these settings proved problematic. They resulted in the USV taking shortcuts that brought it too close to obstacles, sometimes leading to collisions. Here I set them to zero:\\n\"}],\"\\n\",[\"$\",\"div\",\"div-20\",{\"id\":\"test\",\"style\":{\"marginTop\":\"20px\",\"display\":\"flex\",\"justifyContent\":\"space-between\"},\"children\":[\"\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"/avns_assets/Nav2_params_DWB.yaml.png\",\"alt\":\"code\",\"style\":{\"width\":\"700px\",\"maxWidth\":\"none\",\"borderRadius\":\"2px\"}}],\"\\n\"]}],\"\\n\",[\"$\",\"div\",\"div-21\",{\"id\":\"acknowledgements\",\"style\":{\"marginTop\":\"20px\"},\"children\":\"\\nThis project would not have been possible without the invaluable help of my co-authors, Parker Carlson and Ethan Cline.\\n\"}]]}]}]]],null],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/d4c16f7ed518c08c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}],null]},[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark bg-neutral-950\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L9\",null,{\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":null}]}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$La\"],\"globalErrorComponent\":\"$b\",\"missingSlots\":\"$Wc\"}]]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Create T3 App\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Generated by create-t3-app\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\"}]]\n4:null\n"])</script></body></html>